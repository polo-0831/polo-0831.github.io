<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon_polo-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon_polo-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><link rel="apple-touch-icon" href="/images/icons/apple-touch-icon-180x180_polo.png?v=2.6.2" sizes="180x180"><link rel="mask-icon" href="/%5Bobject%20Object%5D?v=2.6.2" color="#54bcff"><meta name="google-site-verification" content="6eG-hPdNNz2SG3XPKdMB0uqKQFPlmcGdEF22DetYT-k"><meta name="msvalidate.01" content="80AB1134C77D311BFEB96C651B502B15"><meta name="baidu-site-verification" content="code-26TjukOFe2"><meta name="description" content="前言       ​    武汉大学叶茫教授在2022年发布的一篇关于行人重识别Reid综述和展望的论文《Deep Learning for Person Re-Identification: A Survey and Outlook.》 –Mang Ye，行人重识别综述。 ​    本篇论文主要讲述了：  行人重识别领域发展以来的各种实现方法，并举出了它们">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记：Deep Learning for Person Re-identification:A Survey and Outlook（行人重识别综述）">
<meta property="og:url" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Po1o&#39;s Mind Palace">
<meta property="og:description" content="前言       ​    武汉大学叶茫教授在2022年发布的一篇关于行人重识别Reid综述和展望的论文《Deep Learning for Person Re-Identification: A Survey and Outlook.》 –Mang Ye，行人重识别综述。 ​    本篇论文主要讲述了：  行人重识别领域发展以来的各种实现方法，并举出了它们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604175417280.png">
<meta property="og:image" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604175952635.png">
<meta property="og:image" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604180451609.png">
<meta property="og:image" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604181220879.png">
<meta property="article:published_time" content="2022-06-04T09:53:58.000Z">
<meta property="article:modified_time" content="2022-06-04T10:18:21.065Z">
<meta property="article:author" content="PoLo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604175417280.png"><title>论文笔记：Deep Learning for Person Re-identification:A Survey and Outlook（行人重识别综述） | Po1o's Mind Palace</title><link ref="canonical" href="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Po1o's Mind Palace</div><div class="header-banner-info__subtitle">日月升落，总有黎明 --杨绛</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">论文笔记：Deep Learning for Person Re-identification:A Survey and Outlook（行人重识别综述）</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-06-04</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-06-04</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon"><i class="fas fa-eye"></i></span><span class="post-meta-item__info">阅读次数</span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body">
        <h1 id="前言">
          <a href="#前言" class="heading-link"><i class="fas fa-link"></i></a><a href="#前言" class="headerlink" title="前言"></a>前言</h1>
      <p>​    武汉大学叶茫教授在2022年发布的一篇关于行人重识别Reid综述和展望的论文<strong>《Deep Learning for Person Re-Identification: A Survey and Outlook.》 –Mang Ye，行人重识别综述</strong>。</p>
<p>​    本篇论文主要讲述了：</p>
<ul>
<li><p>行人重识别领域发展以来的各种实现方法，并举出了它们的优缺点</p>
</li>
<li><p>总结Reid任务由五部分流程构成</p>
</li>
<li><p><strong>提出Close-world和Open-world两类型的Reid工作</strong></p>
</li>
<li><p>新的评价指标mINP；新的baseline：基于AGW的单-多模态reid</p>
</li>
</ul>
<p>这篇论文有利于行人重识别的初学者大致了解该领域先前的工作，这篇文章更加侧重Open-world类型的多模态行人重识别工作，也是目前困难比较多的行人重识别方向。</p>

        <h2 id="计算机视觉论文中的Baseline">
          <a href="#计算机视觉论文中的Baseline" class="heading-link"><i class="fas fa-link"></i></a><a href="#计算机视觉论文中的Baseline" class="headerlink" title="计算机视觉论文中的Baseline"></a>计算机视觉论文中的Baseline</h2>
      <p>​    在计算机视觉领域中的算法创新，都要有一个标准来进行比较才能知道新算法是否创新。</p>
<ul>
<li><p><strong>Benchmark</strong>：一个算法被称为Benchmark，是因为它的是因为它的性能已经被广泛研究，人们对它性能的表现形式、测量方法都非常熟悉，因此可以作为标准方法来衡量其他方法的好坏。简单来说创新的指标标准要好过benchmark的标准，一般新算法用SOTA（state-of-the-art）的算法作为benchmark是最好的。</p>
</li>
<li><p><strong>Baseline</strong>：一个算法被称为baseline，基本上表示比这个算法性能还差的基本上不能接受的，除非方法上有革命性的创新点，而且还有巨大的改进空间和超越benchmark的潜力，只是因为是发展初期而性能有限。所以baseline有一个自带的含义就是“性能起点”。这里还需要指出其另一个应用语境，就是在算法优化过程中，一般version1.0是作为baseline的，即这是你的算法能达到的一个基本性能，在算法继续优化和调参数的过程中，你的目标是比这个性能更好，因此需要在这个baseline的基础上往上跳。</p>
</li>
<li><p>简而言之，benchmark一般是和同行中比较牛的算法比较，比牛算法还好，那你可以考虑发好一点的会议/期刊；baseline一般是自己算法优化和调参过程中自己和自己比较，目标是越来越好，当性能超过benchmark时，可以发表了，当性能甚至超过SOTA时，恭喜你，考虑投顶会顶刊啦。</p>
<p>  参考链接：<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41088475/article/details/105756552">https://blog.csdn.net/qq_41088475/article/details/105756552</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  </p>
</li>
</ul>

        <h1 id="0-摘要">
          <a href="#0-摘要" class="heading-link"><i class="fas fa-link"></i></a><a href="#0-摘要" class="headerlink" title="0.摘要"></a>0.摘要</h1>
      <p>1）通过行人重识别领域的研究，将该领域分为封闭世界（closed-world）和开放世界（open-world）两大类研究。<br>2）封闭世界：<strong>深度特征表示学习</strong>，<strong>深度度量学习</strong>和<strong>排名优化</strong>。封闭世界的研究成果逐渐达到饱和，研究重心自然落在开放世界上，可用五个方面总结其研究。<br>3）提出名为<strong>AGW</strong>的baseline，引入针对ReID的新评价指标<strong>mINP</strong>。</p>

        <h1 id="1-Introduction">
          <a href="#1-Introduction" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1>
      <p>原文：Re-ID is a challenging task due to the presence of <strong>different viewpoints</strong>, <strong>varying low-image resolutions</strong> , <strong>illumination changes</strong>, <strong>unconstrained poses</strong> , <strong>occlusions</strong>, <strong>heterogeneous modalities</strong> , <strong>complex camera environments</strong>, <strong>background clutter</strong> , <strong>unreliable bounding box generations</strong>, etc. These result in varying variations and uncertainty. </p>
<p>目前reid（主要是开放世界）的研究困难主要在<strong>不同视角、参差不齐的低分辨率图像、光照变化、姿态不同、遮挡情况、异构模态数据</strong></p>

        <h2 id="针对特定场景构成reid系统需要五个步骤">
          <a href="#针对特定场景构成reid系统需要五个步骤" class="heading-link"><i class="fas fa-link"></i></a><a href="#针对特定场景构成reid系统需要五个步骤" class="headerlink" title="针对特定场景构成reid系统需要五个步骤"></a>针对特定场景构成reid系统需要五个步骤</h2>
      <p><img src="/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604175417280.png" alt="image-20220604175417280"></p>
<p>1）<strong>原始数据收集</strong>：从处于不同环境的不同地方的摄像机获取原始视频数据。这些数据包含大量的背景杂波。<br>2）<strong>边界框（Bounding Box）生成</strong>：通过行人检测或跟踪算法从原始视频数据中提取包含行人图像的边界框。在大规模应用中不可能手动裁剪所有行人图像。<br>3）<strong>训练数据标注</strong>：对于区分行人任务来说，图像标注必不可少。<br>4）<strong>模型构建和训练</strong>：已经开发了广泛运用的模型，重点在于特征表示学习、度量学习或两者结合。<br>5）<strong>实践测试阶段</strong>：给定一个query和一组gallery，使用上一阶段训练完毕的模型进行行人特征提取，计算query图像和gallery图像的相似度进行排序。</p>

        <h2 id="封闭世界和开放世界的五大差别有">
          <a href="#封闭世界和开放世界的五大差别有" class="heading-link"><i class="fas fa-link"></i></a><a href="#封闭世界和开放世界的五大差别有" class="headerlink" title="封闭世界和开放世界的五大差别有"></a>封闭世界和开放世界的五大差别有</h2>
      <p>1、<strong>单模态和异构数据</strong>（Single-modality Data vs. Heterogeneous Data）：对于步骤1中的原始数据收集，默认所有行人都是在可见光单模态下进行拍摄的，但是在实际的开放世界中，数据可能是异构的，例如，行人可能是在不同光谱、草图、深度图像相机所捕获，甚至可能是文本描述。（这也是本博客关注的重点，即跨模态行人重识别）</p>
<p>2、<strong>边界框生成和原始图像/视频</strong>（Bounding Box Generation vs. Raw Images/Videos）：封闭世界中的行人重识别通常基于边界框提取的行人图像或视频进行训练和测试。但是在实际开放世界中需要直接从原始图像/视频中进行端到端的行人检索。</p>
<p>3、<strong>丰富的标签数据和不可用/有限的标签</strong>（Sufficient Annotated Data vs. Unavailable/Limited Labels）：封闭世界中，行人图像都是标注好的。但在实际应用中，标注费时费力成本高。故引发了有监督和无监督领域。</p>
<p>4、<strong>正确标签和噪声标签</strong>（Correct Annotation vs. Noisy Annotation）：现有的封闭世界的行人重识别领域通常假定所有标签清晰且正确。然而实际应用中，标签噪声和不完善正确的检索跟踪结果导致的样本噪声也都不可避免，故引出了不同噪声类型下的鲁棒行人ReID。</p>
<p>5、<strong>query是否存在于gallery中</strong>（Query Exists in Gallery vs. Open-set）：现有的封闭世界行人ReID都假设查询必须存在于图库中，并计算CMC和mAP。但是在现实情况中，查询行人不一定出现在图库中</p>

        <h2 id="批注：Query和Gallery">
          <a href="#批注：Query和Gallery" class="heading-link"><i class="fas fa-link"></i></a><a href="#批注：Query和Gallery" class="headerlink" title="批注：Query和Gallery"></a>批注：Query和Gallery</h2>
      <p>行人重识别的数据集与普通的视觉检测数据集不同，包括<strong>训练集、验证集、Query和Gallery</strong>，其中训练集和验证集是模型训练时候使用的，query是模型的输入数据，一张行人boundingbox叫prob，多个prob称为query。Gallery则是模型内存储的行人数据，prob就是要在gallery内寻找匹配的行人并且给出rank。</p>
<p><img src="/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604175952635.png" alt="image-20220604175952635"></p>

        <h1 id="2-Closed-world-Re-ID（封闭世界）">
          <a href="#2-Closed-world-Re-ID（封闭世界）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-Closed-world-Re-ID（封闭世界）" class="headerlink" title="2.Closed-world Re-ID（封闭世界）"></a>2.Closed-world Re-ID（封闭世界）</h1>
      <p>此设置通常具有以下假设：<strong>（即与开放世界的五大差别）</strong></p>
<p>​    1）通过单模态可见光摄像机捕获行人。</p>
<p>​    2）已经给出行人bounding box。</p>
<p>​    3）有足够的标注好的训练数据。用于监督训练。</p>
<p>​    4）标签通常是正确的。</p>
<p>​    5）query行人必须出现在图库中。</p>
<p>通常来说CW的ReID包括三个主要组件：</p>

        <h2 id="2-1-Feature-Representation-Learning（特征表示学习）">
          <a href="#2-1-Feature-Representation-Learning（特征表示学习）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-Feature-Representation-Learning（特征表示学习）" class="headerlink" title="2.1 Feature Representation Learning（特征表示学习）"></a>2.1 Feature Representation Learning（特征表示学习）</h2>
      <p><img src="/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604180451609.png" alt="image-20220604180451609"></p>

        <h3 id="2-1-1-Global-Feature-Representation-Learning（全局特征）">
          <a href="#2-1-1-Global-Feature-Representation-Learning（全局特征）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-1-Global-Feature-Representation-Learning（全局特征）" class="headerlink" title="2.1.1 Global Feature Representation Learning（全局特征）"></a>2.1.1 Global Feature Representation Learning（全局特征）</h3>
      <p>将同一个行人的所有图像视为同一类（同一个ID），提取全局特征，将ReID任务视为一个多分类问题。<br>注意力机制也被广泛应用以增强特征表示学习。1）行人图像中attention 2）跨多行人注意力</p>

        <h3 id="2-1-2-Local-Feature-Representation-Learning（局部特征）">
          <a href="#2-1-2-Local-Feature-Representation-Learning（局部特征）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-2-Local-Feature-Representation-Learning（局部特征）" class="headerlink" title="2.1.2 Local Feature Representation Learning（局部特征）"></a>2.1.2 Local Feature Representation Learning（局部特征）</h3>
      <p>具体方案不在此赘述，基本思路即在全局特征之外附加局部特征信息，必定能通过更多信息获得更好的结果。但有些分块技术需要额外的人体姿态对齐策略辅助，而固定分块技术（例如PCB）通过水平划分来进行分块，对重度遮挡和背景杂波敏感。</p>

        <h3 id="2-1-3-Auxiliary-Feature-Representation-Learning（辅助特征）">
          <a href="#2-1-3-Auxiliary-Feature-Representation-Learning（辅助特征）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-3-Auxiliary-Feature-Representation-Learning（辅助特征）" class="headerlink" title="2.1.3 Auxiliary Feature Representation Learning（辅助特征）"></a>2.1.3 Auxiliary Feature Representation Learning（辅助特征）</h3>
      <p>通常需要附加额外的注释信息，可以是文字注释（<strong>semantic attributes</strong>），或利用GAN生成额外的训练样本进行原有数据的扩充（<strong>GAN Generation</strong>）；除此之外还有<strong>Domain Information</strong>（DGD算法-Domain Guided Dropout）和<strong>Viewpoint Information</strong>（MLFN-Multi Level Factorisation Net）。</p>

        <h3 id="2-1-4-Video-Feature-Representation-Learning-视频特征">
          <a href="#2-1-4-Video-Feature-Representation-Learning-视频特征" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-1-4-Video-Feature-Representation-Learning-视频特征" class="headerlink" title="2.1.4 Video Feature Representation Learning(视频特征)"></a>2.1.4 Video Feature Representation Learning(视频特征)</h3>
      
        <h2 id="2-2-Deep-Metric-Learning（深度度量学习）">
          <a href="#2-2-Deep-Metric-Learning（深度度量学习）" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-Deep-Metric-Learning（深度度量学习）" class="headerlink" title="2.2 Deep Metric Learning（深度度量学习）"></a>2.2 Deep Metric Learning（深度度量学习）</h2>
      
        <h3 id="2-2-1-Loss-Function-Design损失函数的设计">
          <a href="#2-2-1-Loss-Function-Design损失函数的设计" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-2-1-Loss-Function-Design损失函数的设计" class="headerlink" title="2.2.1 Loss Function Design损失函数的设计"></a>2.2.1 Loss Function Design损失函数的设计</h3>
      <p><img src="/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/image-20220604181220879.png" alt="image-20220604181220879"></p>
<p>如图分为三种：<strong>交叉熵损失函数（简单的多分类）、验证损失（对比损失）、三元组损失</strong></p>

        <h2 id="3-Open-World-Person-ReID（开放世界）">
          <a href="#3-Open-World-Person-ReID（开放世界）" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-Open-World-Person-ReID（开放世界）" class="headerlink" title="3.Open-World Person ReID（开放世界）"></a>3.Open-World Person ReID（开放世界）</h2>
      <p>3.1 Heterogeneous Re-ID (多模态行人重识别)</p>
<p>3.2 End-to-End Re-ID </p>
<p>3.3 Semi-supervised and Unsupervise Re-ID</p>
<p>3.4 Noise-Robust Re-ID</p>
<p>3.5 Open-set Re-ID and Beyond</p>

        <h2 id="4-AN-OUTLOOK-RE-ID-IN-NEXT-ERA">
          <a href="#4-AN-OUTLOOK-RE-ID-IN-NEXT-ERA" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-AN-OUTLOOK-RE-ID-IN-NEXT-ERA" class="headerlink" title="4.AN OUTLOOK: RE-ID IN NEXT ERA"></a>4.AN OUTLOOK: RE-ID IN NEXT ERA</h2>
      <ul>
<li><strong>新的评价指标mINP</strong></li>
<li><strong>AGW baseline</strong></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://polo-0831.github.io">PoLo</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">https://polo-0831.github.io/2022/06/04/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><nav class="post-paginator paginator"><div class="paginator-next"><a class="paginator-next__link" href="/2022/04/16/windows-anaconda%E9%85%8D%E7%BD%AEpytorch-gpu%E7%8E%AF%E5%A2%83/"><span class="paginator-prev__text">【环境配置】windows+anaconda配置pytorch+gpu环境</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">
          前言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84Baseline"><span class="toc-number">1.1.</span> <span class="toc-text">
          计算机视觉论文中的Baseline</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0-%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">
          0.摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-number">3.</span> <span class="toc-text">
          1.Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E7%89%B9%E5%AE%9A%E5%9C%BA%E6%99%AF%E6%9E%84%E6%88%90reid%E7%B3%BB%E7%BB%9F%E9%9C%80%E8%A6%81%E4%BA%94%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.1.</span> <span class="toc-text">
          针对特定场景构成reid系统需要五个步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%81%E9%97%AD%E4%B8%96%E7%95%8C%E5%92%8C%E5%BC%80%E6%94%BE%E4%B8%96%E7%95%8C%E7%9A%84%E4%BA%94%E5%A4%A7%E5%B7%AE%E5%88%AB%E6%9C%89"><span class="toc-number">3.2.</span> <span class="toc-text">
          封闭世界和开放世界的五大差别有</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E6%B3%A8%EF%BC%9AQuery%E5%92%8CGallery"><span class="toc-number">3.3.</span> <span class="toc-text">
          批注：Query和Gallery</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Closed-world-Re-ID%EF%BC%88%E5%B0%81%E9%97%AD%E4%B8%96%E7%95%8C%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">
          2.Closed-world Re-ID（封闭世界）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Feature-Representation-Learning%EF%BC%88%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">
          2.1 Feature Representation Learning（特征表示学习）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-Global-Feature-Representation-Learning%EF%BC%88%E5%85%A8%E5%B1%80%E7%89%B9%E5%BE%81%EF%BC%89"><span class="toc-number">4.1.1.</span> <span class="toc-text">
          2.1.1 Global Feature Representation Learning（全局特征）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-Local-Feature-Representation-Learning%EF%BC%88%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%EF%BC%89"><span class="toc-number">4.1.2.</span> <span class="toc-text">
          2.1.2 Local Feature Representation Learning（局部特征）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-Auxiliary-Feature-Representation-Learning%EF%BC%88%E8%BE%85%E5%8A%A9%E7%89%B9%E5%BE%81%EF%BC%89"><span class="toc-number">4.1.3.</span> <span class="toc-text">
          2.1.3 Auxiliary Feature Representation Learning（辅助特征）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-4-Video-Feature-Representation-Learning-%E8%A7%86%E9%A2%91%E7%89%B9%E5%BE%81"><span class="toc-number">4.1.4.</span> <span class="toc-text">
          2.1.4 Video Feature Representation Learning(视频特征)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Deep-Metric-Learning%EF%BC%88%E6%B7%B1%E5%BA%A6%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0%EF%BC%89"><span class="toc-number">4.2.</span> <span class="toc-text">
          2.2 Deep Metric Learning（深度度量学习）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-Loss-Function-Design%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.2.1.</span> <span class="toc-text">
          2.2.1 Loss Function Design损失函数的设计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Open-World-Person-ReID%EF%BC%88%E5%BC%80%E6%94%BE%E4%B8%96%E7%95%8C%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">
          3.Open-World Person ReID（开放世界）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-AN-OUTLOOK-RE-ID-IN-NEXT-ERA"><span class="toc-number">4.4.</span> <span class="toc-text">
          4.AN OUTLOOK: RE-ID IN NEXT ERA</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/safari-pinned-tab_polo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">猛兽总是独行，牛羊才成群结队</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/polo-0831/" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://www.youtube.com/channel/UCSy4ymfDX5srRJKCs2RXcMA" target="_blank" rel="noopener" data-popover="Youtube" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-youtube"></i></span></a><a class="sidebar-ov-social-item" href="https://space.bilibili.com/151399898?spm_id_from=333.1007.0.0" target="_blank" rel="noopener" data-popover="social.bilibili" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-youtube"></i></span></a><a class="sidebar-ov-social-item" href="172312551" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">4</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">1</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>PoLo</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__info">访问人数</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__info">浏览总量</span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="300" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1.0.1/dist/canvas-nest.min.js" color="255,20,147" opacity="0.6" count="99" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/gh/sukkaw/busuanzi@latest/bsz.pure.mini.js" async></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body></html>